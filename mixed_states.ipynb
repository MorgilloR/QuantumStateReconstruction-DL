{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b413388d",
   "metadata": {},
   "source": [
    "# **<font color='brown'> State Reconstruction of mixed states</font>**\n",
    "\n",
    "Here, we consider our neural network-based approach for reconstructing mixed states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1675de",
   "metadata": {},
   "source": [
    "# **<font color='brown'> Libraries and Random Seeds</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62d03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import qiskit as qk\n",
    "import qiskit.visualization\n",
    "\n",
    "from qiskit import Aer\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.quantum_info import Statevector\n",
    "\n",
    "from qiskit.providers.aer import AerSimulator\n",
    "from qiskit.providers.aer.noise import QuantumError, ReadoutError\n",
    "from qiskit import transpile\n",
    "from qiskit.quantum_info.operators import Operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Lambda, concatenate\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d80d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reproducibility\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c855ad2",
   "metadata": {},
   "source": [
    "# **<font color='brown'> Functions</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51247cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_unitary(N):\n",
    "    \"\"\"\n",
    "    Generation of an NxN random unitary matrix.\n",
    "    \"\"\"\n",
    "    Z=np.random.randn(N,N) + 1.0j * np.random.randn(N,N)\n",
    "    [Q,R]=sp.linalg.qr(Z)\n",
    "    D=np.diag(np.diagonal(R)/np.abs(np.diagonal(R)))\n",
    "    return np.dot(Q,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c482166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of the simulator\n",
    "sim_bknd=Aer.get_backend('aer_simulator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5c5f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Haar_data(num_qubits, samples=1000):\n",
    "    \"\"\"\n",
    "    Generate \"samples\" Haar distributed data, for \"num_qubits\"- qubit states.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in range(samples):\n",
    "        qc = qk.QuantumCircuit(num_qubits) # create a quantum circuit with \"num_qubits\" qubits\n",
    "        u = random_unitary(2**num_qubits)\n",
    "        qc.unitary(u, qubits=range(num_qubits)) # apply the random unitary transformation to the circuit\n",
    "        qc = qk.transpile(qc, backend=sim_bknd) # optimize the circuit\n",
    "        qc.save_statevector() # save the state vector obtained by the simulation\n",
    "\n",
    "        state = sim_bknd.run(qc).result().get_statevector(qc) # simulation and get the state vector\n",
    "        state = np.asarray(state)\n",
    "        data.append(state) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2251c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mixed states from pure ones\n",
    "def mixed_states(bloch_vectors):\n",
    "   for i in range(len(bloch_vectors)):\n",
    "      bloch_vectors[i] = np.random.rand()**(1/3)* bloch_vectors[i]\n",
    "   return bloch_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4726ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pauli basis for one-qubit states\n",
    "I_tf = tf.constant([1.0, 0.0,0.0,1.0],shape=(2,2), dtype = tf.complex64)\n",
    "X_tf = tf.constant([0.0, 1.0, 1.0, 0.0],shape=(2,2), dtype = tf.complex64)\n",
    "Y_tf = tf.constant([0.0+0j, 0.0-1j ,0.0+1j,0.0+0j],shape=(2,2), dtype = tf.complex64)\n",
    "Z_tf = tf.constant([1.0, 0.0,0.0,-1.0],shape=(2,2), dtype = tf.complex64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b4b95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the density matrix given the state vector\n",
    "def get_density_matrix(state_vector):\n",
    "    density_matrix = np.outer(state_vector, np.conjugate(state_vector))\n",
    "    return density_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a46544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the identity matrix and the Pauli matrices for dimension 2 (one qubit)\n",
    "\n",
    "I = np.array([[1, 0],[0, 1]])\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Y = np.array([[0, -1j], [1j, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "\n",
    "# Function which computes the components of the Bloch vector, given the density matrix \n",
    "\n",
    "def Bloch_vector(rho):\n",
    "    \"\"\"\n",
    "    Compute the components of the Bloch vector, given the density matrix rho:\n",
    "      r_i = Tr[rho * i].\n",
    "    \"\"\"\n",
    "    ax = np.trace(np.dot(rho, X)).real\n",
    "    ay = np.trace(np.dot(rho, Y)).real\n",
    "    az = np.trace(np.dot(rho, Z)).real\n",
    "    pnt = [ax, ay, az]\n",
    "    return np.array(pnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2440ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute density matrix from Bloch vector\n",
    "def rho_from_vec(vector):\n",
    "  el = X*vector[0]+Y*vector[1]+Z*vector[2]\n",
    "  rho = 0.5 *(el + I_tf)\n",
    "  return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e9c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pauli basis without Identity matrix\n",
    "A = tf.stack([X_tf,Y_tf,Z_tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c0afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our loss function\n",
    "@tf.function\n",
    "def inf(a,b):\n",
    "   a = tf.cast(a, tf.complex64)\n",
    "   b = tf.cast(b, tf.complex64)\n",
    "   el_a = tf.einsum('ijk,mi->mjk',A,a) \n",
    "   el_b = tf.einsum('ijk,mi->mjk',A,b) \n",
    "   rho_a = 0.5 *(el_a + I_tf)\n",
    "   rho_b = 0.5 * (el_b +I_tf)\n",
    "\n",
    "\n",
    "   numer = tf.math.abs(tf.linalg.trace(tf.matmul(rho_a, rho_b)))\n",
    "   den = tf.math.sqrt(tf.linalg.trace(tf.linalg.matmul(rho_a, rho_a)) * tf.linalg.trace(tf.linalg.matmul(rho_b, rho_b)))\n",
    "   numer = tf.cast(numer, tf.complex64)\n",
    "   infidelity = 1 - numer/den\n",
    "   return infidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d1a820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fidelity between two density matrices\n",
    "def fidelity_function(a,b):\n",
    "  fid=np.trace(sp.linalg.sqrtm(sp.linalg.sqrtm(a) @ b @ sp.linalg.sqrtm(a))) ** 2\n",
    "  return fid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df93ad",
   "metadata": {},
   "source": [
    "# **<font color='brown'> Noise Models</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5818c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single element of the operator-sum representation\n",
    "\n",
    "def sum_element(rho,operator):\n",
    "    \"\"\"\n",
    "    Single element of the operator-sum representation:\n",
    "    e_i = E_i rho E_i^dagger.\n",
    "    \"\"\"\n",
    "    element = np.dot(np.dot(operator,rho),operator.conj().T)\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba349b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define phase flip channel\n",
    "\n",
    "def phase_flip_error(rho, p):\n",
    "\n",
    "    E_0 = np.sqrt(1-p)*I\n",
    "    E_1 = np.sqrt(p)*Z\n",
    "    \n",
    "    rho_with_flip_error = sum_element(rho, E_0)+sum_element(rho, E_1)\n",
    "    \n",
    "    return rho_with_flip_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a573ed",
   "metadata": {},
   "source": [
    "# **<font color='brown'>Training</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c017682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "data = generate_Haar_data(1, 100)\n",
    "\n",
    "pure_density_matrix_noise_free = [*map(get_density_matrix, data)] #pure haar distributed states obtained\n",
    "pure_vectors_noise_free = [*map(Bloch_vector, pure_density_matrix_noise_free)]\n",
    "bloch_vectors_noise_free = mixed_states(pure_vectors_noise_free) #transform pure states in mixed ones\n",
    "\n",
    "# Compute noise-free density matrix\n",
    "density_matrix_noise_free = [*map(rho_from_vec, bloch_vectors_noise_free)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a88dd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply noise (in this case phase channel with p=0.2)\n",
    "density_matrix_with_noise = []\n",
    "for i in range(len(data)):\n",
    "    single_data_with_noise = phase_flip_error(density_matrix_noise_free[i], 0.2)\n",
    "    density_matrix_with_noise.append(single_data_with_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3393b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute noisy Bloch vectors\n",
    "bloch_vectors_with_noise = [*map(Bloch_vector, density_matrix_with_noise)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c764d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Dataset\n",
    "x_train_list, x_val_list, x_test_list = bloch_vectors_with_noise[:30], bloch_vectors_with_noise[60:80], bloch_vectors_with_noise[80:]\n",
    "y_train_list, y_val_list, y_test_list = bloch_vectors_noise_free[:30], bloch_vectors_noise_free[60:80], bloch_vectors_noise_free[80:]\n",
    "\n",
    "# Convert to tensors\n",
    "x_train = tf.convert_to_tensor(x_train_list)\n",
    "y_train = tf.convert_to_tensor(y_train_list)\n",
    "\n",
    "x_val = tf.convert_to_tensor(x_val_list)\n",
    "y_val = tf.convert_to_tensor(y_val_list)\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test_list)\n",
    "y_test = tf.convert_to_tensor(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4208071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the norms\n",
    "norms_train = tf.norm(y_train, axis=1)\n",
    "norms_val = tf.norm(y_val, axis=1)\n",
    "norms_test = tf.norm(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "845487eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer for normalizing each vector at its fixed length\n",
    "@tf.function\n",
    "def custom_layer(input):\n",
    "  a = tf.gather(input, [0,1,2], axis=1)\n",
    "  b = tf.gather(input, [3], axis=1)\n",
    "  output = a *b\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcab6b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           256         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " vector (Dense)                 (None, 3)            195         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 3)            0           ['vector[0][0]']                 \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4)            0           ['lambda[0][0]',                 \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 3)            0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,611\n",
      "Trainable params: 4,611\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer1 = Input(shape=(3,))\n",
    "input_layer2 = Input(shape=(1,))\n",
    "\n",
    "# First branch\n",
    "x = Dense(64, activation='relu')(input_layer1)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "dense_1 = Dense(units='3', name='vector')(x) \n",
    "normal = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(dense_1)\n",
    "model_1 = Model(inputs = input_layer1, outputs = normal) \n",
    "\n",
    "\n",
    "# Combine the outputs \n",
    "combined = concatenate([model_1.output, input_layer2])\n",
    "\n",
    "# last passage: normalization \n",
    "z = Lambda(custom_layer)(combined)\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_layer1, input_layer2], outputs=z)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9e865",
   "metadata": {},
   "source": [
    "# **<font color='brown'>Infidelity as loss</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a89ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 2s 75ms/step - loss: 0.1987 - val_loss: 0.0682\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0543 - val_loss: 0.0290\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0227 - val_loss: 0.0207\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0143 - val_loss: 0.0118\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0066 - val_loss: 0.0072\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0038 - val_loss: 0.0053\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0029 - val_loss: 0.0040\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0021 - val_loss: 0.0028\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 9.5439e-04 - val_loss: 0.0027\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.6717e-04 - val_loss: 0.0027\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 6.5859e-04 - val_loss: 0.0026\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5.5217e-04 - val_loss: 0.0024\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.9818e-04 - val_loss: 0.0024\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.1704e-04 - val_loss: 0.0023\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3.8268e-04 - val_loss: 0.0022\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 3.2601e-04 - val_loss: 0.0022\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.9733e-04 - val_loss: 0.0021\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.6212e-04 - val_loss: 0.0021\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.3088e-04 - val_loss: 0.0021\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.0765e-04 - val_loss: 0.0021\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.8119e-04 - val_loss: 0.0020\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.6035e-04 - val_loss: 0.0020\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.5281e-04 - val_loss: 0.0019\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.3519e-04 - val_loss: 0.0019\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.2719e-04 - val_loss: 0.0019\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.0917e-04 - val_loss: 0.0018\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.0256e-04 - val_loss: 0.0018\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 9.3961e-05 - val_loss: 0.0018\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 8.6037e-05 - val_loss: 0.0017\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 8.0899e-05 - val_loss: 0.0017\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 7.3767e-05 - val_loss: 0.0017\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.8013e-05 - val_loss: 0.0017\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.2273e-05 - val_loss: 0.0017\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.8993e-05 - val_loss: 0.0017\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.6271e-05 - val_loss: 0.0016\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.1345e-05 - val_loss: 0.0016\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.0030e-05 - val_loss: 0.0016\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.4413e-05 - val_loss: 0.0016\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.1574e-05 - val_loss: 0.0015\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3.8616e-05 - val_loss: 0.0016\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3.6192e-05 - val_loss: 0.0015\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.4861e-05 - val_loss: 0.0015\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.1952e-05 - val_loss: 0.0015\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.9826e-05 - val_loss: 0.0015\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.8225e-05 - val_loss: 0.0015\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.7676e-05 - val_loss: 0.0015\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.4192e-05 - val_loss: 0.0014\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.2737e-05 - val_loss: 0.0014\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.1237e-05 - val_loss: 0.0014\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.0274e-05 - val_loss: 0.0014\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.9779e-05 - val_loss: 0.0014\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.7691e-05 - val_loss: 0.0014\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.7144e-05 - val_loss: 0.0014\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.6222e-05 - val_loss: 0.0014\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.4800e-05 - val_loss: 0.0014\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.4426e-05 - val_loss: 0.0014\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.3663e-05 - val_loss: 0.0014\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.2459e-05 - val_loss: 0.0014\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.1611e-05 - val_loss: 0.0013\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.1398e-05 - val_loss: 0.0013\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.0141e-05 - val_loss: 0.0013\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 9.6222e-06 - val_loss: 0.0013\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 9.6242e-06 - val_loss: 0.0013\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.6665e-06 - val_loss: 0.0013\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.9771e-06 - val_loss: 0.0013\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.6354e-06 - val_loss: 0.0013\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.4883e-06 - val_loss: 0.0013\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.1665e-06 - val_loss: 0.0013\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.3678e-06 - val_loss: 0.0013\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.1413e-06 - val_loss: 0.0013\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.0201e-06 - val_loss: 0.0013\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.3167e-06 - val_loss: 0.0013\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.9412e-06 - val_loss: 0.0013\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.0684e-06 - val_loss: 0.0013\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.4127e-06 - val_loss: 0.0013\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.5319e-06 - val_loss: 0.0013\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.4386e-06 - val_loss: 0.0013\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 3.5564e-06 - val_loss: 0.0013\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 3.6697e-06 - val_loss: 0.0013\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4531e-06 - val_loss: 0.0013\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5187e-06 - val_loss: 0.0013\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.0835e-06 - val_loss: 0.0012\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9564e-06 - val_loss: 0.0012\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8749e-06 - val_loss: 0.0012\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4259e-06 - val_loss: 0.0012\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4796e-06 - val_loss: 0.0012\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1398e-06 - val_loss: 0.0012\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.1100e-06 - val_loss: 0.0012\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.9729e-06 - val_loss: 0.0012\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7564e-06 - val_loss: 0.0012\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8954e-06 - val_loss: 0.0012\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4722e-06 - val_loss: 0.0012\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4623e-06 - val_loss: 0.0012\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4822e-06 - val_loss: 0.0012\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4206e-06 - val_loss: 0.0012\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3073e-06 - val_loss: 0.0012\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.2219e-06 - val_loss: 0.0012\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1067e-06 - val_loss: 0.0012\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0153e-06 - val_loss: 0.0012\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5765e-07 - val_loss: 0.0012\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7023e-07 - val_loss: 0.0012\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.3844e-07 - val_loss: 0.0012\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.5566e-07 - val_loss: 0.0012\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7685e-07 - val_loss: 0.0012\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7486e-07 - val_loss: 0.0012\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.6757e-07 - val_loss: 0.0012\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7220e-07 - val_loss: 0.0012\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9406e-07 - val_loss: 0.0012\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.1459e-07 - val_loss: 0.0012\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.6624e-07 - val_loss: 0.0012\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.2452e-07 - val_loss: 0.0012\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4.2319e-07 - val_loss: 0.0012\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2518e-07 - val_loss: 0.0012\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7352e-07 - val_loss: 0.0012\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.8544e-07 - val_loss: 0.0012\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.5564e-07 - val_loss: 0.0012\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2385e-07 - val_loss: 0.0012\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.0001e-07 - val_loss: 0.0012\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4173e-07 - val_loss: 0.0012\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8610e-07 - val_loss: 0.0012\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.7418e-07 - val_loss: 0.0012\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.5431e-07 - val_loss: 0.0012\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7617e-07 - val_loss: 0.0012\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4041e-07 - val_loss: 0.0012\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.6226e-07 - val_loss: 0.0012\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.0663e-07 - val_loss: 0.0012\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.0862e-07 - val_loss: 0.0012\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.8875e-07 - val_loss: 0.0012\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7484e-07 - val_loss: 0.0012\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5497e-07 - val_loss: 0.0012\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5100e-07 - val_loss: 0.0012\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4901e-07 - val_loss: 0.0012\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2318e-07 - val_loss: 0.0012\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.1126e-07 - val_loss: 0.0012\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.2716e-07 - val_loss: 0.0012\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1.1921e-07 - val_loss: 0.0012\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.3113e-07 - val_loss: 0.0012\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7420e-08 - val_loss: 0.0012\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.2318e-07 - val_loss: 0.0012\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9341e-08 - val_loss: 0.0012\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.1394e-08 - val_loss: 0.0012\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0331e-07 - val_loss: 0.0012\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.9473e-08 - val_loss: 0.0012\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3644e-08 - val_loss: 0.0012\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1591e-08 - val_loss: 0.0012\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.7552e-08 - val_loss: 0.0012\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.7552e-08 - val_loss: 0.0012\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5763e-08 - val_loss: 0.0012\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.1657e-08 - val_loss: 0.0012\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1591e-08 - val_loss: 0.0012\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.1657e-08 - val_loss: 0.0012\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.3578e-08 - val_loss: 0.0012\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5499e-08 - val_loss: 0.0012\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1591e-08 - val_loss: 0.0012\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.5565e-08 - val_loss: 0.0012\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9605e-08 - val_loss: 0.0012\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.1723e-08 - val_loss: 0.0012\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.3710e-08 - val_loss: 0.0012\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5631e-08 - val_loss: 0.0012\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.5631e-08 - val_loss: 0.0012\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.1657e-08 - val_loss: 0.0012\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.7750e-08 - val_loss: 0.0012\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5763e-08 - val_loss: 0.0012\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3710e-08 - val_loss: 0.0012\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3842e-08 - val_loss: 0.0012\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5763e-08 - val_loss: 0.0012\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1855e-08 - val_loss: 0.0012\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.5829e-08 - val_loss: 0.0012\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5763e-08 - val_loss: 0.0012\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7816e-08 - val_loss: 0.0012\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7816e-08 - val_loss: 0.0012\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9868e-08 - val_loss: 0.0012\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9671e-08 - val_loss: 0.0012\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5895e-08 - val_loss: 0.0012\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.3710e-08 - val_loss: 0.0012\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9868e-08 - val_loss: 0.0012\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1855e-08 - val_loss: 0.0012\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.7750e-08 - val_loss: 0.0012\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1921e-08 - val_loss: 0.0012\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.9341e-09 - val_loss: 0.0012\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5895e-08 - val_loss: 0.0012\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3908e-08 - val_loss: 0.0012\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5.9605e-09 - val_loss: 0.0012\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.1921e-08 - val_loss: 0.0012\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.7684e-08 - val_loss: 0.0012\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.1789e-08 - val_loss: 0.0012\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.1789e-08 - val_loss: 0.0012\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.9341e-09 - val_loss: 0.0012\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1855e-08 - val_loss: 0.0012\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5895e-08 - val_loss: 0.0012\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.1921e-08 - val_loss: 0.0012\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 7.9473e-09 - val_loss: 0.0012\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5.9605e-09 - val_loss: 0.0012\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9605e-09 - val_loss: 0.0012\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: -9.9341e-09 - val_loss: 0.0012\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: -9.9341e-09 - val_loss: 0.0012\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1855e-08 - val_loss: 0.0012\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.9605e-09 - val_loss: 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Define Loss\n",
    "adam_opt = tf.optimizers.Adam(0.001)\n",
    "# Compile model\n",
    "model.compile(optimizer=adam_opt, \n",
    "              loss=inf)\n",
    "\n",
    "history = model.fit([x_train, norms_train], y_train, validation_data=([x_val, norms_val], y_val), batch_size=5, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f528c63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 7.9109e-04 - 226ms/epoch - 226ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([x_test, norms_test], y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6e8198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity between ideal and predicted Bloch vectors: (0.9994213560907068-5.041319191652715e-09j)\n"
     ]
    }
   ],
   "source": [
    "#save the model predictions in a tensor\n",
    "y_prediction = model([x_test, norms_test])\n",
    "y_prediction = tf.cast(y_prediction, tf.complex64)\n",
    "y_test = tf.cast(y_test, tf.complex64)\n",
    "\n",
    "\n",
    "fidelities = []\n",
    "for i in range(len(y_prediction)):\n",
    "  den_mat = rho_from_vec(y_prediction[i])\n",
    "  den_mat = np.asarray(den_mat)\n",
    "\n",
    "  den_mat_id = rho_from_vec(y_test[i])\n",
    "  den_mat_id = np.asarray(den_mat_id)\n",
    "\n",
    "  fidelity = fidelity_function(den_mat_id, den_mat)\n",
    "  fidelities.append(fidelity)\n",
    "\n",
    "\n",
    "print(f\"Fidelity between ideal and predicted Bloch vectors: {tf.math.reduce_mean(fidelities).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e3495",
   "metadata": {},
   "source": [
    "# **<font color='brown'>MSE as loss</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13ac4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 46ms/step - loss: 5.4105e-04 - val_loss: 0.0021\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.3281e-04 - val_loss: 0.0012\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.8277e-04 - val_loss: 0.0012\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.6931e-04 - val_loss: 9.9850e-04\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.3582e-04 - val_loss: 0.0011\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.2669e-05 - val_loss: 8.1494e-04\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.8562e-05 - val_loss: 0.0010\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1196e-05 - val_loss: 8.9468e-04\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.9655e-05 - val_loss: 9.4323e-04\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4390e-05 - val_loss: 9.0377e-04\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3.2854e-05 - val_loss: 7.6248e-04\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.8576e-05 - val_loss: 9.7931e-04\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5888e-05 - val_loss: 7.2130e-04\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.7537e-05 - val_loss: 8.3099e-04\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.0799e-05 - val_loss: 7.9101e-04\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5129e-05 - val_loss: 9.0634e-04\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7327e-05 - val_loss: 7.9649e-04\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.0833e-05 - val_loss: 7.7952e-04\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2439e-05 - val_loss: 8.1302e-04\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3645e-05 - val_loss: 8.6243e-04\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4567e-05 - val_loss: 8.2693e-04\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6856e-05 - val_loss: 8.3340e-04\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.1930e-06 - val_loss: 8.1666e-04\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4321e-05 - val_loss: 7.5842e-04\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.0514e-05 - val_loss: 8.2875e-04\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9711e-05 - val_loss: 8.4510e-04\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.1521e-05 - val_loss: 7.3030e-04\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.5588e-05 - val_loss: 8.3744e-04\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.1902e-05 - val_loss: 9.4099e-04\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.0021e-04 - val_loss: 0.0011\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.0978e-04 - val_loss: 8.7026e-04\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4.8183e-04 - val_loss: 0.0012\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.9307e-04 - val_loss: 7.0482e-04\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.7490e-04 - val_loss: 9.5388e-04\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.8510e-04 - val_loss: 8.3285e-04\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.4369e-04 - val_loss: 0.0012\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.1688e-04 - val_loss: 0.0013\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.9656e-04 - val_loss: 0.0013\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.4202e-04 - val_loss: 9.1198e-04\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.0525e-04 - val_loss: 8.2405e-04\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.4145e-04 - val_loss: 6.6042e-04\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.0856e-04 - val_loss: 7.6754e-04\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.2165e-05 - val_loss: 5.0813e-04\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.3468e-05 - val_loss: 5.7958e-04\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.0156e-05 - val_loss: 6.2264e-04\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.6283e-05 - val_loss: 6.8221e-04\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1.1670e-04 - val_loss: 5.7071e-04\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.4921e-05 - val_loss: 5.9590e-04\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.8951e-05 - val_loss: 6.2357e-04\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.6881e-05 - val_loss: 5.1108e-04\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.1534e-05 - val_loss: 6.0889e-04\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.2567e-05 - val_loss: 5.4636e-04\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.8910e-05 - val_loss: 4.9971e-04\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 8.4696e-05 - val_loss: 5.7982e-04\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.7278e-05 - val_loss: 4.6124e-04\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.1081e-05 - val_loss: 7.2771e-04\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.6002e-04 - val_loss: 5.8365e-04\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.8199e-04 - val_loss: 9.1517e-04\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.5994e-04 - val_loss: 5.9323e-04\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.7125e-04 - val_loss: 6.5314e-04\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.6747e-04 - val_loss: 7.6370e-04\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.5825e-04 - val_loss: 5.1341e-04\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.8889e-04 - val_loss: 0.0012\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.2499e-04 - val_loss: 6.2805e-04\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.3104e-04 - val_loss: 4.8582e-04\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.8101e-04 - val_loss: 8.2523e-04\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.0014e-04 - val_loss: 5.8126e-04\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.1967e-04 - val_loss: 5.2044e-04\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.3961e-05 - val_loss: 6.9126e-04\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.9006e-05 - val_loss: 4.0889e-04\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.5320e-05 - val_loss: 4.3883e-04\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 5.4993e-05 - val_loss: 6.6435e-04\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.6398e-05 - val_loss: 4.1652e-04\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5757e-05 - val_loss: 4.4909e-04\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8624e-05 - val_loss: 5.4453e-04\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8900e-05 - val_loss: 4.9068e-04\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.3520e-05 - val_loss: 5.0315e-04\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.6214e-05 - val_loss: 5.0866e-04\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.7911e-06 - val_loss: 4.8414e-04\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.1513e-06 - val_loss: 4.6523e-04\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.7632e-06 - val_loss: 4.7841e-04\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.1545e-06 - val_loss: 4.8048e-04\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 6.7651e-06 - val_loss: 5.1825e-04\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4315e-05 - val_loss: 4.8643e-04\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.6473e-06 - val_loss: 4.6577e-04\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.0578e-06 - val_loss: 4.6491e-04\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.7855e-06 - val_loss: 4.9803e-04\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.7498e-06 - val_loss: 4.8052e-04\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.1809e-06 - val_loss: 4.7626e-04\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.6062e-06 - val_loss: 4.9388e-04\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.6559e-06 - val_loss: 4.9003e-04\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.9532e-06 - val_loss: 4.9047e-04\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 5.0763e-06 - val_loss: 4.7385e-04\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 6.4154e-06 - val_loss: 4.7093e-04\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.3413e-06 - val_loss: 4.8858e-04\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.4486e-06 - val_loss: 4.8315e-04\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9585e-06 - val_loss: 5.2341e-04\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.6652e-06 - val_loss: 4.6344e-04\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.6717e-06 - val_loss: 4.9008e-04\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4511e-06 - val_loss: 4.9872e-04\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1259e-06 - val_loss: 4.8258e-04\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5008e-06 - val_loss: 4.8625e-04\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6134e-06 - val_loss: 5.0884e-04\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0313e-06 - val_loss: 4.6792e-04\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5637e-06 - val_loss: 4.8797e-04\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4.7386e-06 - val_loss: 4.6097e-04\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.5485e-06 - val_loss: 4.8289e-04\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.3412e-06 - val_loss: 4.9038e-04\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.3061e-06 - val_loss: 4.7912e-04\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4114e-06 - val_loss: 4.7387e-04\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2704e-06 - val_loss: 5.0553e-04\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5362e-05 - val_loss: 4.9070e-04\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4194e-05 - val_loss: 5.2815e-04\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3295e-05 - val_loss: 5.5067e-04\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6880e-05 - val_loss: 4.7731e-04\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8295e-05 - val_loss: 5.0682e-04\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2209e-05 - val_loss: 4.5932e-04\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2976e-05 - val_loss: 4.7092e-04\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9977e-06 - val_loss: 5.2247e-04\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5171e-05 - val_loss: 4.7876e-04\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2122e-05 - val_loss: 4.7257e-04\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.1899e-05 - val_loss: 4.3842e-04\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0516e-05 - val_loss: 4.8907e-04\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0375e-05 - val_loss: 4.8501e-04\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9346e-06 - val_loss: 4.7635e-04\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4251e-05 - val_loss: 4.6351e-04\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0705e-05 - val_loss: 4.9372e-04\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4087e-05 - val_loss: 4.7272e-04\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4413e-06 - val_loss: 4.7842e-04\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5961e-06 - val_loss: 4.7044e-04\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.4803e-06 - val_loss: 4.6475e-04\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9551e-06 - val_loss: 4.8178e-04\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2458e-06 - val_loss: 4.7553e-04\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.0704e-06 - val_loss: 4.6636e-04\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6094e-06 - val_loss: 4.7987e-04\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.2088e-06 - val_loss: 4.9386e-04\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.1887e-05 - val_loss: 4.8697e-04\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.6578e-05 - val_loss: 4.7521e-04\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 9.0043e-06 - val_loss: 4.5069e-04\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 7.5897e-06 - val_loss: 4.6473e-04\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.2845e-05 - val_loss: 4.8166e-04\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.8833e-05 - val_loss: 4.5120e-04\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1201e-05 - val_loss: 4.5459e-04\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.0343e-05 - val_loss: 5.4446e-04\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9810e-05 - val_loss: 5.1066e-04\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.5026e-05 - val_loss: 5.4421e-04\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.4431e-05 - val_loss: 5.0173e-04\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.3622e-05 - val_loss: 5.7317e-04\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9.2230e-05 - val_loss: 5.0476e-04\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.8273e-05 - val_loss: 5.9985e-04\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1.1759e-04 - val_loss: 7.2219e-04\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.4860e-04 - val_loss: 5.0888e-04\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 11ms/step - loss: 9.2945e-05 - val_loss: 6.2036e-04\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6576e-04 - val_loss: 7.9263e-04\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.6871e-04 - val_loss: 6.2800e-04\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.8330e-04 - val_loss: 9.4182e-04\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3.8567e-04 - val_loss: 6.5836e-04\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.3763e-04 - val_loss: 7.4026e-04\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.5502e-04 - val_loss: 6.3988e-04\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2.3701e-04 - val_loss: 7.6788e-04\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.1735e-04 - val_loss: 0.0011\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4877e-04 - val_loss: 6.7441e-04\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5276e-04 - val_loss: 4.3962e-04\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7164e-04 - val_loss: 5.0390e-04\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.4569e-04 - val_loss: 5.2001e-04\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.9175e-04 - val_loss: 6.7018e-04\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1.5036e-04 - val_loss: 4.3842e-04\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1818e-04 - val_loss: 5.1401e-04\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2055e-04 - val_loss: 4.5423e-04\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.7443e-05 - val_loss: 3.6510e-04\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8127e-05 - val_loss: 3.4307e-04\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5.0795e-05 - val_loss: 3.9820e-04\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9580e-05 - val_loss: 3.7058e-04\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4545e-05 - val_loss: 3.7861e-04\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7961e-05 - val_loss: 3.9102e-04\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.6330e-05 - val_loss: 3.5808e-04\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.2856e-05 - val_loss: 3.9417e-04\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.2290e-05 - val_loss: 3.6237e-04\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.3999e-05 - val_loss: 3.4665e-04\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.9089e-05 - val_loss: 4.3400e-04\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.9113e-05 - val_loss: 4.5816e-04\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.1164e-05 - val_loss: 5.6282e-04\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.8393e-05 - val_loss: 4.9478e-04\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 8.7533e-05 - val_loss: 4.5439e-04\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.9364e-05 - val_loss: 5.3867e-04\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1.1606e-04 - val_loss: 4.7659e-04\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1597e-04 - val_loss: 4.8598e-04\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4005e-04 - val_loss: 5.4010e-04\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.5126e-05 - val_loss: 3.8703e-04\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4589e-04 - val_loss: 3.6769e-04\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.4017e-05 - val_loss: 4.1039e-04\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.7603e-05 - val_loss: 3.4926e-04\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8857e-05 - val_loss: 3.9493e-04\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2526e-05 - val_loss: 3.8788e-04\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5997e-05 - val_loss: 4.1373e-04\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3.1130e-05 - val_loss: 3.5266e-04\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2.1974e-05 - val_loss: 3.6682e-04\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7281e-05 - val_loss: 3.5530e-04\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.6542e-05 - val_loss: 4.3176e-04\n"
     ]
    }
   ],
   "source": [
    "# Define Loss\n",
    "adam_opt = tf.optimizers.Adam(0.001)\n",
    "# Compile model\n",
    "model.compile(optimizer=adam_opt, \n",
    "              loss=inf)\n",
    "\n",
    "history = model.fit([x_train, norms_train], y_train, validation_data=([x_val, norms_val], y_val), batch_size=5, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5654e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 3.3815e-04 - 220ms/epoch - 220ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate([x_test, norms_test], y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48089b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fidelity between ideal and predicted Bloch vectors: (0.9997519706146927-4.957207442575716e-09j)\n"
     ]
    }
   ],
   "source": [
    "#save the model predictions in a tensor\n",
    "y_prediction = model([x_test, norms_test])\n",
    "y_prediction = tf.cast(y_prediction, tf.complex64)\n",
    "y_test = tf.cast(y_test, tf.complex64)\n",
    "\n",
    "\n",
    "fidelities = []\n",
    "for i in range(len(y_prediction)):\n",
    "  den_mat = rho_from_vec(y_prediction[i])\n",
    "  den_mat = np.asarray(den_mat)\n",
    "\n",
    "  den_mat_id = rho_from_vec(y_test[i])\n",
    "  den_mat_id = np.asarray(den_mat_id)\n",
    "\n",
    "  fidelity = fidelity_function(den_mat_id, den_mat)\n",
    "  fidelities.append(fidelity)\n",
    "\n",
    "print(f\"Fidelity between ideal and predicted Bloch vectors: {tf.math.reduce_mean(fidelities).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d15c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative fidelity\n",
    "@tf.function\n",
    "def inf_vec(a,b):\n",
    "    fid = 0.5 * (1 + tf.tensordot(a, b, 1) + ((1 - tf.tensordot(a, a, 1))*(1 - tf.tensordot(b, b, 1)))**(1/2))\n",
    "    return 1 - fid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
